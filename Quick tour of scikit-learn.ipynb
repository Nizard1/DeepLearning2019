{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feature_descriptions(filename):\n",
    "    names = []\n",
    "    types = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            if l[0] == '|' or ':' not in l:\n",
    "                continue\n",
    "            cols = l.split(':')\n",
    "            names.append(cols[0])\n",
    "            if cols[1].startswith(' continuous.'):\n",
    "                types.append(float)\n",
    "            else:\n",
    "                types.append(str)\n",
    "    return names, types\n",
    "\n",
    "feat_names, feat_types = read_feature_descriptions('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/adult.names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'education-num',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'native-country']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[float,\n",
       " str,\n",
       " float,\n",
       " str,\n",
       " float,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " float,\n",
       " float,\n",
       " float,\n",
       " str]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, feat_names, feat_types):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            cols = l.strip('\\n.').split(', ')\n",
    "            if len(cols) < len(feat_names): # skip empty lines and comments\n",
    "                continue\n",
    "            X.append( { n:t(c) for n, t, c in zip(feat_names, feat_types, cols) } )\n",
    "            Y.append(cols[-1])\n",
    "    return X, Y\n",
    "\n",
    "Xtrain, Ytrain = read_data('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/adult.data', feat_names, feat_types)\n",
    "Xtest, Ytest = read_data('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/adult.test', feat_names, feat_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# or use the Pandas library to read the data. \n",
    "import pandas as pd\n",
    "\n",
    "def df_to_dicts(df):\n",
    "    return [ { col:x for col, x in zip(df.columns, row) } for row in df.values]\n",
    "\n",
    "traindata = pd.read_csv('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/adult.data', header=None, sep=', ', engine='python', names=feat_names+['target'])\n",
    "testdata = pd.read_csv('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/adult.test', header=None, sep=', ', engine='python', skiprows=1, names=feat_names+['target'])\n",
    "\n",
    "Xtrain = df_to_dicts(traindata.drop('target',axis=1))\n",
    "Ytrain = traindata['target']\n",
    "Xtest = df_to_dicts(testdata.drop('target', axis=1))\n",
    "Ytest = testdata['target'].map(lambda s: s[:-1]) # remove period character at the end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Tree-based classifiers:\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "## or \n",
    "# import sklearn.tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Neural network classifier\n",
    "\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from  sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "### Linear classifiers:\n",
    "#from sklearn.svm import LinearSVC\n",
    "#from sklearn.linear_model import Perceptron\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    \n",
    "    \n",
    "    ### Tree-based classifiers\n",
    "    #RandomForestClassifier()\n",
    "    #DecisionTreeClassifier()\n",
    "    ## or \n",
    "    #sklearn.tree.DecisionTreeClassifier()\n",
    "    GradientBoostingClassifier()\n",
    "\n",
    "    \n",
    "    ### Linear classifiers:\n",
    "    #LinearSVC()\n",
    "    #Perceptron()\n",
    "    #LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Neural network classifier (will take longer time to train):\n",
    "    \n",
    "    ### The linear and neural network classifiers will perform better\n",
    "    ### if you add a \n",
    "    ### sklearn.preprocessing.StandardScaler(with_mean=False)\n",
    "    ### to the pipeline, after the vectorizer but before the classifier. \n",
    "    \n",
    "    #StandardScaler(with_mean=False),\n",
    "    #MLPClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nizar\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([3.32940936, 3.22801757, 3.15949607]), 'score_time': array([0.15763688, 0.15757871, 0.15658593]), 'test_score': array([0.8620785 , 0.86493459, 0.86998986])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "print(cross_validate(pipeline, Xtrain, Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8712609790553406\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(Xtrain, Ytrain)\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print(accuracy_score(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.47044000e+04 4.83268000e+03 3.28650000e-01 2.22950000e+02\n",
      "  2.02666469e+06 2.57585000e+02 5.34750000e+03 2.34000000e+02\n",
      "  2.43080000e+01]\n",
      " [9.38740000e+03 2.15629000e+03 2.29700000e-01 1.10018000e+02\n",
      "  1.30353141e+06 1.35064000e+02 4.19480000e+03 5.90000000e+01\n",
      "  3.38637000e+01]\n",
      " [7.55055000e+03 1.93553000e+03 2.56340000e-01 8.31325000e+01\n",
      "  1.05843401e+06 1.23905000e+02 3.09674000e+03 5.00000000e+00\n",
      "  3.76889000e+01]\n",
      " [8.25861000e+03 2.35777000e+03 2.85490000e-01 9.05637000e+01\n",
      "  1.16747470e+06 1.26941000e+02 3.96787000e+03 1.65000000e+02\n",
      "  3.40915000e+01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "alldata = np.loadtxt('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/CASP.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "Yall = alldata[:,0]\n",
    "Xall = alldata[:,1:]\n",
    "### Unlike the Adult dataset, this dataset does not have a\n",
    "### standard train/test split. We use the scikit-learn function\n",
    "### train_test_split to split the data into 80% for training \n",
    "### and 20% for testing. \n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(Xall, Yall, train_size=0.8)\n",
    "\n",
    "#print(Xtrain[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  alternative solution that uses Pandas instead of NumPy\n",
    "import pandas as pd\n",
    "alldata = pd.read_csv('C:/Users/Nizar/Documents/Studing/Deep Learning HT19 Umeå University/Applied Machine Learning, 2019/Dataset/CASP.csv')\n",
    "Yall = alldata['RMSD']\n",
    "Xall = alldata.drop('RMSD', axis=1)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(Xall, Yall, train_size=0.8)\n",
    "#print(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nizar\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Nizar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Nizar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Nizar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([14.92210746, 16.07499266, 16.39915943]),\n",
       " 'score_time': array([0.0229373 , 0.02991819, 0.02692723]),\n",
       " 'test_score': array([-21.95707188, -21.60302749, -21.51530901])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from  sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=False),\n",
    "    \n",
    "    #LinearRegression()\n",
    "    #Ridge()\n",
    "    #Lasso()\n",
    "    #DecisionTreeRegressor()\n",
    "    #RandomForestRegressor()\n",
    "    #GradientBoostingRegressor()\n",
    "    MLPRegressor()\n",
    ")\n",
    "cross_validate(pipeline, Xtrain, Ytrain, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nizar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.67512344670125"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "  \n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "mean_squared_error(Ytest, pipeline.predict(Xtest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nizar'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
